{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We assume that all the necessary packages are already installed as explained in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation (QG) Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datasets\n",
    "from cliffs_delta import cliffs_delta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "import evaluate\n",
    "from scipy.stats import shapiro, kruskal, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_question_model_data_from_file(prefix, metric):\n",
    "    \"\"\"\n",
    "    Load the question model data from the file.\n",
    "    \n",
    "    :param prefix: the prefix of the file\n",
    "    :param metric: the metric to load\n",
    "    :return: the data\n",
    "    \"\"\"\n",
    "    filename = f\"./question_models_results/{prefix}_score_{metric}.json\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data['scores']\n",
    "\n",
    "def load_question_model_data(model_name, score_metrics):\n",
    "    \"\"\"\n",
    "    Load the question model data from the file.\n",
    "        \n",
    "    :param model_name: the name of the model\n",
    "    :param score_metrics: the metrics to load\n",
    "    \"\"\"\n",
    "    first_metric_data = load_question_model_data_from_file(model_name, score_metrics[0])\n",
    "    data = pd.DataFrame(first_metric_data, columns=[score_metrics[0]])\n",
    "\n",
    "    for metric in score_metrics[1:]:\n",
    "        next_metric_data = load_question_model_data_from_file(model_name, metric)\n",
    "        data[metric] = next_metric_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_metrics = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'bleurt', 'bertscore']\n",
    "potsawee_question = load_question_model_data('potsawee', score_metrics)\n",
    "allenai_question = load_question_model_data('allenai', score_metrics)\n",
    "custom_question = load_question_model_data('custom', score_metrics)\n",
    "custom_with_answer_question = load_question_model_data('custom_with_answer', score_metrics)\n",
    "\n",
    "def plot_score_metrics(data1, data2, data3, data4, title, metrics):\n",
    "    \"\"\"\n",
    "    Plot the score metrics as histograms.\n",
    "    \n",
    "    :param data1: the first data\n",
    "    :param data2: the second data\n",
    "    :param data3: the third data\n",
    "    :param data4: the fourth data\n",
    "    :param title: the title of the plot\n",
    "    :param metrics: the metrics to plot\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    fig.supxlabel('Score', fontsize=14)\n",
    "    fig.supylabel('Frequency', fontsize=14)\n",
    "    for i, metric in enumerate(metrics):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        ax[row, col].hist(data1[metric], label='potsawee', alpha=0.5)\n",
    "        ax[row, col].hist(data2[metric], label='allenai', alpha=0.5)\n",
    "        ax[row, col].hist(data3[metric], label='custom', alpha=0.5)\n",
    "        ax[row, col].hist(data4[metric], label='custom_with_answer', alpha=0.5)\n",
    "        ax[row, col].set_title(metric)\n",
    "        ax[row, col].legend()\n",
    "plot_score_metrics(potsawee_question.dropna(), allenai_question.dropna(), custom_question.dropna(), custom_with_answer_question.dropna(), 'Comparison metric', score_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Statistical Analysis of QG models\n",
    "Cliff's delta is an effect size statistic appropriate in cases where a Wilcoxon-Mann-Whitney test might be used. It ranges from -1 to 1, with 0 indicating stochastic equality, and 1 indicating that the first group dominates the second. Currently, the function makes no provisions for NA values in the data. It is recommended that NAs be removed beforehand.\n",
    "\n",
    "When the data in the first group are greater than in the second group, Cliff's delta is positive. When the data in the second group are greater than in the first group, Cliff's delta is negative.\n",
    "\n",
    "[source](https://search.r-project.org/CRAN/refmans/rcompanion/html/cliffDelta.html)\n",
    "\n",
    "## Bleu\n",
    "BLEU’s output is always a number between 0 and 1. This value indicates how similar the candidate text is to the reference texts, with values closer to 1 representing more similar texts. Few human translations will attain a score of 1, since this would indicate that the candidate is identical to one of the reference translations. For this reason, it is not necessary to attain a score of 1. Because there are more opportunities to match, adding additional reference translations will increase the BLEU score.\n",
    "\n",
    "[source](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
    "\n",
    "## Rouge\n",
    "ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference.\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/ROUGE_(metric))\n",
    "\n",
    "## Bleurt\n",
    "BLEURT’s output is always a number between 0 and (approximately 1). This value indicates how similar the generated text is to the reference texts, with values closer to 1 representing more similar texts.\n",
    "\n",
    "[source](https://huggingface.co/spaces/evaluate-metric/bleurt)\n",
    "\n",
    "## Bertscore\n",
    "BERTScore (Precision, Recall, F1) scores lie between the range of 0 and 1, with 0 representing no semantic similarity, and 1 representing a perfect semantic match between candidate and reference texts.\n",
    "\n",
    "[source](https://docs.kolena.com/metrics/bertscore/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shapiro-Wilk')\n",
    "for metric in score_metrics:\n",
    "    stat_potsawee, p_potsawee = shapiro(potsawee_question[metric].dropna())\n",
    "    stat_allenai, p_allenai = shapiro(allenai_question[metric].dropna())\n",
    "    stat_custom, p_custom = shapiro(custom_question[metric].dropna())\n",
    "    stat_custom_with_answer, p_custom_with_answer = shapiro(custom_with_answer_question[metric].dropna())\n",
    "    print(f\"{metric} - Potsawee: {stat_potsawee}, {p_potsawee}, AllenAI: {stat_allenai}, {p_allenai}, Custom: {stat_custom}, {p_custom}\")\n",
    "\n",
    "print('Median')\n",
    "for metric in score_metrics:\n",
    "    median_potsawee = potsawee_question[metric].median()\n",
    "    median_allenai = allenai_question[metric].median()\n",
    "    median_custom = custom_question[metric].median()\n",
    "    median_custom_with_answer = custom_with_answer_question[metric].median()\n",
    "    print(f\"{metric} - Potsawee: {median_potsawee}, AllenAI: {median_allenai}, Custom: {median_custom}, Custom with answer: {median_custom_with_answer}\")\n",
    "    \n",
    "print('Kruskal-Wallis')\n",
    "for metric in score_metrics:\n",
    "    stat, p = kruskal(potsawee_question[metric].dropna(), allenai_question[metric].dropna(), custom_question[metric].dropna(), custom_with_answer_question[metric].dropna())\n",
    "    print(f\"{metric} - {stat}, {p}\")\n",
    "\n",
    "print('Cliff\\'s Delta')\n",
    "for metric in score_metrics:\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"potsawee vs allenai: {cliffs_delta(potsawee_question[metric].dropna(), allenai_question[metric].dropna())}\")\n",
    "    print(f\"potsawee vs custom: {cliffs_delta(potsawee_question[metric].dropna(), custom_question[metric].dropna())}\")\n",
    "    print(f\"potsawee vs custom_with_answer: {cliffs_delta(potsawee_question[metric].dropna(), custom_with_answer_question[metric].dropna())}\")\n",
    "    print(f\"allenai vs custom: {cliffs_delta(allenai_question[metric].dropna(), custom_question[metric].dropna())}\")\n",
    "    print(f\"allenai vs custom_with_answer: {cliffs_delta(allenai_question[metric].dropna(), custom_with_answer_question[metric].dropna())}\")\n",
    "    print(f\"custom vs custom_with_answer: {cliffs_delta(custom_question[metric].dropna(), custom_with_answer_question[metric].dropna())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Investigate impact of length of support"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = datasets.load_dataset('allenai/sciq', split='test').to_pandas()\n",
    "test_data['support_length'] = test_data['support'].apply(lambda x: len(x))\n",
    "print('Shapiro-Wilk')\n",
    "stat, p = shapiro(test_data['support_length'])\n",
    "print(f\"Support Length: {stat}, {p}\")\n",
    "\n",
    "print('Spearman')\n",
    "print('Custom model without correct answer as input')\n",
    "non_none_test_data = test_data[custom_question['bleu'].notna()]\n",
    "non_none_data = custom_question['bleu'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs BLEU Custom: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_question['rouge1'].notna()]\n",
    "non_none_data = custom_question['rouge1'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs ROUGE1 Custom: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_question['rouge2'].notna()]\n",
    "non_none_data = custom_question['rouge2'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs ROUGE2 Custom: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_question['rougeL'].notna()]\n",
    "non_none_data = custom_question['rougeL'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs ROUGEL Custom: {stat}, {p}\")\n",
    "\n",
    "\n",
    "non_none_test_data = test_data[custom_question['bleurt'].notna()]\n",
    "non_none_data = custom_question['bleurt'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs BLEURT Custom: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_question['bertscore'].notna()]\n",
    "non_none_data = custom_question['bertscore'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs BERTScore Custom: {stat}, {p}\")\n",
    "\n",
    "print('Custom model with correct answer as input')\n",
    "non_none_test_data = test_data[custom_with_answer_question['bleu'].notna()]\n",
    "non_none_data = custom_with_answer_question['bleu'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs BLEU Custom with answer: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_with_answer_question['rouge1'].notna()]\n",
    "non_none_data = custom_with_answer_question['rouge1'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs ROUGE1 Custom with answer: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_with_answer_question['rouge2'].notna()]\n",
    "non_none_data = custom_with_answer_question['rouge2'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs ROUGE2 Custom with answer: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_with_answer_question['rougeL'].notna()]\n",
    "non_none_data = custom_with_answer_question['rougeL'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs ROUGEL Custom with answer: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_with_answer_question['bleurt'].notna()]\n",
    "non_none_data = custom_with_answer_question['bleurt'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs BLEURT Custom with answer: {stat}, {p}\")\n",
    "\n",
    "non_none_test_data = test_data[custom_with_answer_question['bertscore'].notna()]\n",
    "non_none_data = custom_with_answer_question['bertscore'].dropna()\n",
    "stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "print(f\"Support Length vs BERTScore Custom with answer: {stat}, {p}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Question Answering Model Comparison\n",
    "Preprocess generated answers according to correctness, fuzzy matching, bleu and rouge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_answer_model_data_from_file(model_name):\n",
    "    \"\"\"\n",
    "    Load the answer model data from the file.\n",
    "    \n",
    "    :param model_name: the name of the model\n",
    "    :return: the data as a DataFrame\n",
    "    \"\"\"\n",
    "    filename = f\"./answer_models_results/generated_data_{model_name}.json\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def load_preprocessed_data_from_file(model_name):\n",
    "    \"\"\"\n",
    "    Load the preprocessed data from the file.\n",
    "    \n",
    "    :param model_name: the name of the model\n",
    "    :return: the data as a DataFrame\n",
    "    \"\"\"\n",
    "    filename = f\"./answer_models_results/processed_data_{model_name}.json\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Load the SciQ test dataset.\n",
    "\n",
    "    :return: the SciQ test dataset\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('allenai/sciq', split='test')\n",
    "    return data\n",
    "\n",
    "def add_correctness_column(model_df, data_test_df):\n",
    "    \"\"\"\n",
    "    Add the correctness column based on exact string matching to the model DataFrame.\n",
    "    \n",
    "    :param model_df: the model DataFrame\n",
    "    :param data_test_df: the test data DataFrame\n",
    "    \"\"\"\n",
    "    model_df['is_correct'] = model_df['answer'].eq(data_test_df['correct_answer'])\n",
    "    model_df['is_correct'] = model_df['is_correct'].fillna(False)\n",
    "\n",
    "def add_fuzzy_column(model_df, data_test_df):\n",
    "    \"\"\"\n",
    "    Add the fuzzy column based on fuzzy matching to the model DataFrame.\n",
    "    \n",
    "    :param model_df: the model DataFrame\n",
    "    :param data_test_df: the test data DataFrame\n",
    "    \"\"\"\n",
    "    model_df['fuzzy'] = model_df.apply(lambda row: fuzz.ratio(row['answer'], data_test_df['correct_answer'][row.name]), axis=1)\n",
    "\n",
    "def add_bleu_column(model_df, data_test_df):\n",
    "    \"\"\"\n",
    "    Add the BLEU column based on BLEU score to the model DataFrame.\n",
    "    \n",
    "    :param model_df: the model DataFrame\n",
    "    :param data_test_df: the test data DataFrame\n",
    "    \"\"\"\n",
    "    scorer = evaluate.load('bleu')\n",
    "    model_df['bleu'] = None\n",
    "\n",
    "    for i, row in model_df.iterrows():\n",
    "        if pd.isnull(row['answer']):\n",
    "            continue\n",
    "        score = scorer.compute(predictions=[row['answer']], references=[data_test_df['correct_answer'][i]])\n",
    "        model_df.at[i, 'bleu'] = score['bleu']\n",
    "\n",
    "def add_rouge_columns(model_df, data_test_df):\n",
    "    \"\"\"\n",
    "    Add the ROUGE columns based on ROUGE score to the model DataFrame.\n",
    "    \n",
    "    :param model_df: the model DataFrame\n",
    "    :param data_test_df: the test data DataFrame\n",
    "    \"\"\"\n",
    "    scorer = evaluate.load('rouge')\n",
    "    model_df['rouge1'] = None\n",
    "    model_df['rouge2'] = None\n",
    "    model_df['rougeL'] = None\n",
    "    for i, row in model_df.iterrows():\n",
    "        if pd.isnull(row['answer']):\n",
    "            continue\n",
    "        score = scorer.compute(predictions=[row['answer']], references=[data_test_df['correct_answer'][i]])\n",
    "        model_df.at[i, 'rouge1'] = score['rouge1']\n",
    "        model_df.at[i, 'rouge2'] = score['rouge2']\n",
    "        model_df.at[i, 'rougeL'] = score['rougeL']\n",
    "\n",
    "def preprocessed_files_exist(model_name):\n",
    "    \"\"\"\n",
    "    Check if the preprocessed files exist.\n",
    "    \n",
    "    :param model_name: the name of the model\n",
    "    \"\"\"\n",
    "    filename = f\"./answer_models_results/processed_data_{model_name}.json\"\n",
    "    try:\n",
    "        data = pd.read_json(filename)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def save_processed_data_to_file(model_name, df):\n",
    "    \"\"\"\n",
    "    Save the processed data to the file.\n",
    "    \n",
    "    :param model_name: the name of the model\n",
    "    :param df: the DataFrame to save\n",
    "    \"\"\"\n",
    "    filename = f\"./answer_models_results/processed_data_{model_name}.json\"\n",
    "    df.to_json(filename, orient='records')\n",
    "\n",
    "models_data = {\n",
    "    'deepset': None,\n",
    "    'intel': None,\n",
    "    'distilbert': None\n",
    "}\n",
    "\n",
    "all_exist = True\n",
    "for name in models_data.keys():\n",
    "    all_exist = all_exist and preprocessed_files_exist(name)\n",
    "\n",
    "if not all_exist:\n",
    "    print('Preprocessing data')       \n",
    "    for name, df in models_data.items():\n",
    "        data_test = load_test_data()\n",
    "        add_correctness_column(df, data_test)\n",
    "        add_fuzzy_column(df, data_test)\n",
    "        add_bleu_column(df, data_test)\n",
    "        add_rouge_columns(df, data_test)\n",
    "    for name, df in models_data.items():\n",
    "        save_processed_data_to_file(name, df)\n",
    "else:\n",
    "    for key in models_data.keys():\n",
    "        df = load_preprocessed_data_from_file(key)\n",
    "        models_data[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot metrics of generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correctness(models_data):\n",
    "    \"\"\"\n",
    "    Plot the correctness of the models.\n",
    "    \n",
    "    :param models_data: the models data\n",
    "    \"\"\"\n",
    "    correct_counts = [df['is_correct'].sum() for df in models_data.values()]\n",
    "    none_counts = [df['answer'].isnull().sum() for df in models_data.values()]\n",
    "    \n",
    "    x = range(len(models_data))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    rects1 = ax.bar(x, correct_counts, width, label='Correct Answers')\n",
    "    rects2 = ax.bar([p + width for p in x], none_counts, width, label='None Answers')\n",
    "    \n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_title('Model Answer Comparisons')\n",
    "    ax.set_xticks([p + width / 2 for p in x])\n",
    "    ax.set_xticklabels(models_data.keys())\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_fuzzy(models_data):\n",
    "    \"\"\"\n",
    "    Plot the fuzzy ratio of the models.\n",
    "    \n",
    "    :param models_data: the models data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.boxplot([df['fuzzy'] for df in models_data.values()])\n",
    "    ax.set_xticklabels(models_data.keys())\n",
    "    ax.set_title('Fuzzy Ratio Comparison')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Fuzzy Ratio')\n",
    "    plt.show()\n",
    "\n",
    "def plot_bleu(models_data):\n",
    "    \"\"\"\n",
    "    Plot the BLEU score of the models.\n",
    "    \n",
    "    :param models_data: the models data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.boxplot([df['bleu'].dropna() for df in models_data.values()])\n",
    "    ax.set_xticklabels(models_data.keys())\n",
    "    ax.set_title('BLEU Score Comparison')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('BLEU Score')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rouge1(models_data):\n",
    "    \"\"\"\n",
    "    Plot the ROUGE-1 score of the models.\n",
    "    \n",
    "    :param models_data: the models data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.boxplot([df['rouge1'].dropna() for df in models_data.values()])\n",
    "    ax.set_xticklabels(models_data.keys())\n",
    "    ax.set_title('ROUGE-1 Score Comparison')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('ROUGE-1 Score')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rouge2(models_data):\n",
    "    \"\"\"\n",
    "    Plot the ROUGE-2 score of the models.\n",
    "    \n",
    "    :param models_data: the models data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.boxplot([df['rouge2'].dropna() for df in models_data.values()])\n",
    "    ax.set_xticklabels(models_data.keys())\n",
    "    ax.set_title('ROUGE-2 Score Comparison')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('ROUGE-2 Score')\n",
    "    plt.show()\n",
    "\n",
    "def plot_rougeL(models_data):\n",
    "    \"\"\"\n",
    "    Plot the ROUGE-L score of the models.\n",
    "    \n",
    "    :param models_data: the models data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.boxplot([df['rougeL'].dropna() for df in models_data.values()])\n",
    "    ax.set_xticklabels(models_data.keys())\n",
    "    ax.set_title('ROUGE-L Score Comparison')\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('ROUGE-L Score')\n",
    "    plt.show()\n",
    "plot_correctness(models_data)\n",
    "plot_fuzzy(models_data)\n",
    "plot_bleu(models_data)\n",
    "plot_rouge1(models_data)\n",
    "plot_rouge2(models_data)\n",
    "plot_rougeL(models_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical anaylsis\n",
    "We follow the same approach as for the question generation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shapiro-Wilk')\n",
    "for metric in ['is_correct', 'fuzzy', 'bleu', 'rouge1', 'rouge2', 'rougeL']:\n",
    "    for name, df in models_data.items():\n",
    "        stat, p = shapiro(df[metric].dropna())\n",
    "        print(f\"{metric} - {name}: {stat}, {p}\")\n",
    "print('Median')\n",
    "for metric in ['is_correct', 'fuzzy', 'bleu', 'rouge1', 'rouge2', 'rougeL']:\n",
    "    for name, df in models_data.items():\n",
    "        median = df[metric].median()\n",
    "        print(f\"{metric} - {name}: {median}\")\n",
    "\n",
    "print('Kruskal-Wallis')\n",
    "for metric in ['is_correct', 'fuzzy', 'bleu', 'rouge1', 'rouge2', 'rougeL']:\n",
    "    stat, p = kruskal(models_data['deepset'][metric].dropna(), models_data['intel'][metric].dropna(), models_data['distilbert'][metric].dropna())\n",
    "    print(f\"{metric} - {stat}, {p}\")\n",
    "\n",
    "print('Cliff\\'s Delta')\n",
    "for metric in ['is_correct', 'fuzzy', 'bleu', 'rouge1', 'rouge2', 'rougeL']:\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"deepset vs intel: {cliffs_delta(models_data['deepset'][metric].dropna(), models_data['intel'][metric].dropna())}\")\n",
    "    print(f\"deepset vs distilbert: {cliffs_delta(models_data['deepset'][metric].dropna(), models_data['distilbert'][metric].dropna())}\")\n",
    "    print(f\"intel vs distilbert: {cliffs_delta(models_data['intel'][metric].dropna(), models_data['distilbert'][metric].dropna())}\")\n",
    "\n",
    "print('Number of correct answers')\n",
    "print('Number of correct answers Deepset: ', models_data['deepset']['is_correct'].sum())\n",
    "print('Number of correct answers Intel: ', models_data['intel']['is_correct'].sum())\n",
    "print('Number of correct answer Distilbert: ', models_data['distilbert']['is_correct'].sum())\n",
    "\n",
    "print('Number of none answers')\n",
    "print('Number of none answers Deepset: ', models_data['deepset']['answer'].isnull().sum())\n",
    "print('Number of none answers Intel: ', models_data['intel']['answer'].isnull().sum())\n",
    "print('Number of none answers Distilbert: ', models_data['distilbert']['answer'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## See what resulted in None answer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = load_test_data().to_pandas()\n",
    "none_data = {\n",
    "    'deepset': test_data[models_data['deepset']['answer'].isnull()],\n",
    "    'intel': test_data[models_data['intel']['answer'].isnull()],\n",
    "    'distilbert': test_data[models_data['distilbert']['answer'].isnull()]\n",
    "}\n",
    "\n",
    "for name, df in none_data.items():\n",
    "    print(f\"{name}: {df['support'].apply(lambda x: x == '').sum()}\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Investigate impact of length of support"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = datasets.load_dataset('allenai/sciq', split='test').to_pandas()\n",
    "test_data['support_length'] = test_data['support'].apply(lambda x: len(x))\n",
    "print('Shapiro-Wilk')\n",
    "metrics = ['fuzzy', 'bleu', 'rouge1', 'rouge2', 'rougeL']\n",
    "for metric in metrics:\n",
    "    stat, p = shapiro(models_data['deepset'][metric].dropna())\n",
    "    print(f\"Support Length: {stat}, {p}\")\n",
    "\n",
    "print('Spearman')\n",
    "for metric in metrics:\n",
    "    non_none_test_data = test_data[models_data['deepset'][metric].notna()]\n",
    "    non_none_data = models_data['deepset'][metric].dropna()\n",
    "    stat, p = spearmanr(non_none_test_data['support_length'], non_none_data)\n",
    "    print(f\"Support Length vs {metric} Deepset: {stat}, {p}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractor Generation Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_distractor_model_data_from_file(model_name):\n",
    "    \"\"\"\n",
    "    Load the distractor model data from the file.\n",
    "    \n",
    "    :param model_name: the name of the model\n",
    "    \"\"\"\n",
    "    filename = f\"./distractor_models_results/generated_distractors_{model_name}.json\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "distractors_data = {\n",
    "    'potsawee': None,\n",
    "    'custom1': None,\n",
    "    'custom2': None,\n",
    "    'custom3': None,\n",
    "    'custom4': None\n",
    "}\n",
    "\n",
    "for name in distractors_data.keys():\n",
    "    df = load_distractor_model_data_from_file(name)\n",
    "    distractors_data[name] = df\n",
    "    \n",
    "def plot_successful_distractors(distractors_data):\n",
    "    \"\"\"\n",
    "    Plot the successful distractors of the models.\n",
    "    \n",
    "    :param distractors_data: the distractors data\n",
    "    \"\"\"\n",
    "    total_len = [len(df) for df in distractors_data.values()]\n",
    "    none_counts = np.array([df['bleurt_score'].isnull().sum() for df in distractors_data.values()])\n",
    "    non_none_counts = total_len - none_counts\n",
    "    \n",
    "    x = range(len(distractors_data))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    rects1 = ax.bar(x, non_none_counts, width, label='Successful Distractors')\n",
    "    rects2 = ax.bar([p + width for p in x], none_counts, width, label='None Distractors')\n",
    "    \n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_title('Model Distractor Comparisons')\n",
    "    ax.set_xticks([p + width / 2 for p in x])\n",
    "    ax.set_xticklabels(distractors_data.keys())\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "plot_successful_distractors(distractors_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shapiro-Wilk')\n",
    "for name, df in distractors_data.items():\n",
    "    computed_metrics = df['bleurt_score'].dropna()\n",
    "    if len(computed_metrics) < 3:\n",
    "        print(f\"Skipping {name} as there are not enough samples\")\n",
    "        continue\n",
    "    stat, p = shapiro(computed_metrics)\n",
    "    print(f\"Bleurt - {name}: {stat}, {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Investigate impact of length of support"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = datasets.load_dataset('allenai/sciq', split='test').to_pandas()\n",
    "test_data['support_length'] = test_data['support'].apply(lambda x: len(x))\n",
    "\n",
    "print('Shapiro-Wilk')\n",
    "stat, p = shapiro(distractors_data['potsawee']['bleurt_score'].dropna())\n",
    "print(f\"Bleurt - Potsawee: {stat}, {p}\")\n",
    "\n",
    "print('Spearman')\n",
    "non_null_test_data = test_data[distractors_data['potsawee']['bleurt_score'].notna().index.astype('int32')]\n",
    "stat, p = spearmanr(non_null_test_data, distractors_data['potsawee']['bleurt_score'].dropna())\n",
    "print(f\"Support Length vs BLEURT Potsawee: {stat}, {p}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Mean support length: ', test_data['support_length'].mean())\n",
    "print('Median support length: ', test_data['support_length'].median())\n",
    "print('Max support length: ', test_data['support_length'].max())\n",
    "print('Min support length: ', test_data['support_length'].min())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
