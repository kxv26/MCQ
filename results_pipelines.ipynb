{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro, kruskal, spearmanr, wilcoxon\n",
    "from cliffs_delta import cliffs_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_pipeline_analysis_data(name):\n",
    "    \"\"\"\n",
    "    Load the analysis data for a pipeline\n",
    "    \n",
    "    :param name: the name of the pipeline\n",
    "    :return: a pandas DataFrame with the analysis data\n",
    "    \"\"\"\n",
    "    file_name = f'pipeline_results/analysis_{name}_result.json'\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def load_pipeline_data(name):\n",
    "    \"\"\"\n",
    "    Load the generated data for a pipeline\n",
    "    \n",
    "    :param name: the name of the pipeline\n",
    "    :return: a pandas DataFrame with the generated data\n",
    "    \"\"\"\n",
    "    file_name = f'pipeline_results/{name}_result.json'\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "pipelines_metrics = {\n",
    "    'pipeline1': load_pipeline_analysis_data('pipeline1'),\n",
    "    'pipeline2': load_pipeline_analysis_data('pipeline2'),\n",
    "    'pipeline3': load_pipeline_analysis_data('pipeline3')\n",
    "}\n",
    "\n",
    "pipelines = {\n",
    "    'pipeline1': load_pipeline_data('pipeline1'),\n",
    "    'pipeline2': load_pipeline_data('pipeline2'),\n",
    "    'pipeline3': load_pipeline_data('pipeline3')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creates a sample for manual analysis\n",
    "The sample contains 40 questions from the test set and the generated questions and distractors from pipeline1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline1 = load_pipeline_data('pipeline1')\n",
    "pipeline1.dropna(subset=['generated_question', 'generated_distractors', 'generated_distractors'], inplace=True)\n",
    "pipeline1.drop(columns=['correct_answer', 'support','time'], axis=1, inplace=True)\n",
    "pipeline1_sample = pipeline1.sample(40, random_state=42)\n",
    "\n",
    "pipeline2 = load_pipeline_data('pipeline2')\n",
    "pipeline2.dropna(subset=['generated_question', 'generated_distractors', 'generated_distractors'], inplace=True)\n",
    "pipeline2.drop(columns=['correct_answer', 'support','time'], axis=1, inplace=True)\n",
    "pipeline2_sample = pipeline2.sample(40, random_state=42)\n",
    "\n",
    "test_data = datasets.load_dataset('allenai/sciq', split='test').to_pandas()\n",
    "test_data_sample = test_data.loc[pipeline1_sample.index.astype('int32')]\n",
    "test_data_sample.reset_index(drop=True, inplace=True)\n",
    "pipeline1_sample.reset_index(drop=True, inplace=True)\n",
    "manual_sample_1 = pd.concat([test_data_sample, pipeline1_sample], axis=1)\n",
    "manual_sample_1.to_csv('manual_analysis_1.csv', columns=['question', 'generated_question', 'correct_answer', 'distractor1', 'distractor2', 'distractor3', 'generated_distractors', 'support'])\n",
    "manual_sample_2 = pd.concat([test_data_sample, pipeline2_sample], axis=1)\n",
    "manual_sample_2.to_csv('manual_analysis_2.csv', columns=['question', 'generated_question', 'correct_answer', 'distractor1', 'distractor2', 'distractor3', 'generated_distractors', 'support'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate manual scoring"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "manual_pipe1 = pd.read_csv('pipeline1_manual.csv')\n",
    "manual_pipe2 = pd.read_csv('pipeline2_manual.csv')\n",
    "for column in manual_pipe1.columns[1:]:\n",
    "    manual_pipe1[column] = (manual_pipe1[column] - manual_pipe1[column].mean()) / manual_pipe1[column].std() if manual_pipe1[column].std() != 0 else 0\n",
    "    manual_pipe2[column] = (manual_pipe2[column] - manual_pipe2[column].mean()) / manual_pipe2[column].std() if manual_pipe2[column].std() != 0 else 0\n",
    "    \n",
    "for column in manual_pipe1.columns[1:]:\n",
    "    stat1, p1 = shapiro(manual_pipe1[column])\n",
    "    stat2, p2 = shapiro(manual_pipe2[column])\n",
    "    res = wilcoxon(manual_pipe1[column], manual_pipe2[column], zero_method='zsplit')\n",
    "    print(f'Wilcoxon test for guideline {column}: stat={res.statistic}, p={res.pvalue}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Statistical analysis of metrics for QG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_q_metrics = ['bleu', 'rouge1', 'rouge2', 'rougeL', 'bleurt', 'bertscore']\n",
    "pipeline_a_metrics = ['bleu_ans', 'rouge1_ans', 'rouge2_ans', 'rougeL_ans', 'bertscore_ans']\n",
    "print('Shapiro-Wilk test for normality')\n",
    "for pipeline, df in pipelines_metrics.items():\n",
    "    for metric in pipeline_q_metrics:\n",
    "        stat, p = shapiro(df[metric].dropna())\n",
    "        print(f'{pipeline} {metric} p={p}')\n",
    "print('Kruskal-Wallis H-test for equal medians')\n",
    "for metric in pipeline_q_metrics:\n",
    "    stat, p = kruskal(\n",
    "        pipelines_metrics['pipeline1'][metric].dropna(),\n",
    "        pipelines_metrics['pipeline2'][metric].dropna(),\n",
    "        pipelines_metrics['pipeline3'][metric].dropna(),\n",
    "    )\n",
    "    print(f'{metric}: stat={stat}, p={p}')\n",
    "    \n",
    "print('Cliff\\'s delta for effect size')\n",
    "for metric in pipeline_q_metrics:\n",
    "    for pipeline1, pipeline2 in [('pipeline1', 'pipeline2'), ('pipeline1', 'pipeline3'), ('pipeline2', 'pipeline3')]:\n",
    "        delta = cliffs_delta(pipelines_metrics[pipeline1][metric].dropna(), pipelines_metrics[pipeline2][metric].dropna())\n",
    "        print(f'{pipeline1} vs {pipeline2} {metric}: {delta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis of answer metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.boxplot(pipelines_metrics['pipeline3']['bleu_ans'].dropna(), positions=[1], widths=0.6)\n",
    "plt.boxplot(pipelines_metrics['pipeline3']['rouge1_ans'].dropna(), positions=[2], widths=0.6)\n",
    "plt.boxplot(pipelines_metrics['pipeline3']['rouge2_ans'].dropna(), positions=[3], widths=0.6)\n",
    "plt.boxplot(pipelines_metrics['pipeline3']['rougeL_ans'].dropna(), positions=[4], widths=0.6)\n",
    "plt.boxplot(pipelines_metrics['pipeline3']['bertscore_ans'].dropna(), positions=[5], widths=0.6)\n",
    "plt.xticks([1, 2, 3, 4, 5], ['bleu', 'rouge1', 'rouge2', 'rougeL', 'bertscore'])\n",
    "plt.title('Pipeline3 answer metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis of distractors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.boxplot(pipelines_metrics['pipeline1']['max_bleurt'].dropna(), positions=[1], widths=0.6)\n",
    "plt.boxplot(pipelines_metrics['pipeline2']['max_bleurt'].dropna(), positions=[2], widths=0.6)\n",
    "plt.boxplot(pipelines_metrics['pipeline3']['max_bleurt'].dropna(), positions=[3], widths=0.6)\n",
    "plt.xticks([1, 2, 3], ['pipeline1', 'pipeline2', 'pipeline3'])\n",
    "plt.ylabel('Score')\n",
    "plt.title('Distractors bleurt score')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Shapiro-Wilk test for normality')\n",
    "for pipeline, df in pipelines_metrics.items():\n",
    "    stat, p = shapiro(df['max_bleurt'].dropna())\n",
    "    print(f'{pipeline} max_bleurt p={p}')\n",
    "print('Kruskal-Wallis H-test for equal medians')\n",
    "stat, p = kruskal(\n",
    "    pipelines_metrics['pipeline1']['max_bleurt'].dropna(),\n",
    "    pipelines_metrics['pipeline2']['max_bleurt'].dropna()\n",
    ")\n",
    "print(f'Kruskal-Wallis H-test for equal medians: stat={stat}, p={p}')\n",
    "stat, p = kruskal(\n",
    "    pipelines_metrics['pipeline1']['max_bleurt'].dropna(),\n",
    "    pipelines_metrics['pipeline3']['max_bleurt'].dropna()\n",
    ")\n",
    "print(f'Kruskal-Wallis H-test for equal medians: stat={stat}, p={p}')\n",
    "stat, p = kruskal(\n",
    "    pipelines_metrics['pipeline2']['max_bleurt'].dropna(),\n",
    "    pipelines_metrics['pipeline3']['max_bleurt'].dropna()\n",
    ")\n",
    "print(f'Kruskal-Wallis H-test for equal medians: stat={stat}, p={p}')\n",
    "print('Cliff\\'s delta for effect size')\n",
    "for pipeline1, pipeline2 in [('pipeline1', 'pipeline2'), ('pipeline1', 'pipeline3'), ('pipeline2', 'pipeline3')]:\n",
    "    delta = cliffs_delta(pipelines_metrics[pipeline1]['max_bleurt'].dropna(), pipelines_metrics[pipeline2]['max_bleurt'].dropna())\n",
    "    print(f'{pipeline1} vs {pipeline2}: {delta}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Spearmann correlation between time and bleurt')\n",
    "for name, pipeline in pipelines.items():\n",
    "    time_non_null = pipeline['time'].dropna()\n",
    "    bleurt_non_null = pipelines_metrics[name]['max_bleurt'].dropna()\n",
    "    corr, p = spearmanr(bleurt_non_null, time_non_null)\n",
    "    print(f'{name} corr={corr}, p={p}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Statistical analysis of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Shapiro-Wilk test for normality')\n",
    "for pipeline, df in pipelines.items():\n",
    "    stat, p = shapiro(df['time'].dropna())\n",
    "    print(f'{pipeline} time p={p}')\n",
    "print('Kruskal-Wallis H-test for equal medians')\n",
    "stat, p = kruskal(\n",
    "    pipelines['pipeline1']['time'].dropna(),\n",
    "    pipelines['pipeline2']['time'].dropna(),\n",
    "    pipelines['pipeline3']['time'].dropna(),\n",
    ")\n",
    "print(f'Kruskal-Wallis H-test for equal medians: stat={stat}, p={p}')\n",
    "print('Cliff\\'s delta for effect size')\n",
    "for pipeline1, pipeline2 in [('pipeline1', 'pipeline2'), ('pipeline1', 'pipeline3'), ('pipeline2', 'pipeline3')]:\n",
    "    delta = cliffs_delta(pipelines[pipeline1]['time'].dropna(), pipelines[pipeline2]['time'].dropna())\n",
    "    print(f'{pipeline1} vs {pipeline2}: {delta}')\n",
    "    \n",
    "print('Medians')\n",
    "print('pipeline1 median: ', pipelines['pipeline1']['time'].median())\n",
    "print('pipeline2 median: ', pipelines['pipeline2']['time'].median())\n",
    "print('pipeline3 median: ', pipelines['pipeline3']['time'].median())\n",
    "\n",
    "print('Means')\n",
    "print('pipeline1 mean: ', pipelines['pipeline1']['time'].mean())\n",
    "print('pipeline2 mean: ', pipelines['pipeline2']['time'].mean())\n",
    "print('pipeline3 mean: ', pipelines['pipeline3']['time'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = datasets.load_dataset('allenai/sciq', split='test').to_pandas()\n",
    "test_data['support_length'] = test_data['support'].apply(lambda x: len(x))\n",
    "print('Spearman correlation between time and support length')\n",
    "for name, pipeline in pipelines.items():\n",
    "    time_non_null = pipeline['time'].dropna()\n",
    "    test_data_filtered = test_data.loc[time_non_null.index.astype('int32')]\n",
    "\n",
    "    corr, p = spearmanr(test_data_filtered['support_length'], time_non_null)\n",
    "    print(f'{name} corr={corr}, p={p}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
